# Thoughts on caching algorithms for Khan content data

I wanted to expand on what I said during our meeting, which is that my belief is that an LRU cache is not the most efficient approach for our content data. Specifically, I think an approach like our current "popular items" cache, but with better weighting between languages, makes more efficient use of a fixed amount of space, if we do it well.

## Content access patterns

We know that content access is very unequally distributed: the most frequently accessed items are accessed much more frequently, by multiple orders of magnitude, than the least frequently accessed ones.  (This may be obvious, but I want to say it clearly.)  This is because some courses are more popular than others, but also because some item types need to be loaded in more different contexts; for example we typically need to load all ancestors of a topic (in the relevant topic path) which means that domains and courses are accessed much more than leaf nodes even in the same course.

Additionally, my key assumption is that content access frequency changes slowly over time: the most frequently accessed items now will mostly be the same ones in a few hours, or tomorrow.  This is a bit less obvious; clearly it's not true during big publishes that do "slug swaps" to point popular URLs at new content items!  But those are rare -- like a few times a year rare -- and otherwise I believe most of our traffic is fairly stable from day to day.  There are some other exceptions to this, which I'll discuss below.  We may want to collect some data to verify this.

## Cache algorithms

As usual, the [perfect caching strategy](https://en.wikipedia.org/wiki/Cache_replacement_policies#B%C3%A9l%C3%A1dy's_algorithm) is to know what requests we will receive in the future, and discard the items whose next time of access is the farthest from now.  Of course we can't know that!  My claim is that a semi-static approach gets us very close.

Specifically, the way we get close is that if content access frequency does not change, we *can* know the item for which the *expected value* of the next time of access is the farthest from now: it's the globally least frequently used item.  Now, we don't want our instances to have to communicate with each other to decide what to evict, but we can just compute such data statically, and load it once as a part of the manifest.

This is similar to the algorithm the old Memcached FMS used (for the same reason), and to the "popular items" approach we are now using in Go for English and Spanish.  It suggests a few minor improvements.  First, we should ideally compute the popular items globally across languages (perhaps adjusted by some sort of equity metric to make sure unpopular languages aren't penalized).  Second, if we are a new instance, and the first item we load is the `n+1`st most popular, we may as well keep it in memory until we load the `n` more popular items!  We can accomplish both of these by keeping with each item (perhaps in the manifest) an absolute score of how popular it is, which naively would simply be the fraction of all item-accesses in all languages that are this item; whenever we need to evict we evict the item with the lowest score.

The potential problems with this come from cases where access patterns change more quickly.  One obvious place is between languages: different languages may be in use more or less at different times of day.  We could solve for this, if we feel it is relevant, by keeping track of the most frequently used *languages*, and adjusting each language's usage-scores on that basis.  But of course content access patterns can change in more random ways, such as publishes with "slug swaps".  This suggests using a component of local LFU-based caching might be helpful.  One option is to just do local LFU, but keep track of the access frequency of every item in any active manifest, not just those that we have in memory; this avoids many of the traditional problems of LFU and matches well to a somewhat but not totally stable pattern of accesses.  It seems like dgraph's ristretto does basically this out of the box, so it may be a good option.  But it may be we can decide this simply isn't necessary, if content access pattern changes are fairly small and rare.
